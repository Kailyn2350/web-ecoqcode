// python -m http.server 8000
// ngrok http 8000

const modelPath = "model/ecoq_classifier.onnx";
let session;

const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");

async function initCamera() {
  const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } });
  video.srcObject = stream;
  return new Promise(resolve => video.onloadedmetadata = resolve);
}

async function loadModel() {
  session = await ort.InferenceSession.create(modelPath);
  console.log("ONNX model loaded");
}

function preprocessFrame() {
  const size = 224; // ëª¨ë¸ ì…ë ¥ ì‚¬ì´ì¦ˆ
  const cropSize = Math.min(video.videoWidth, video.videoHeight); // ë¹„ë””ì˜¤ì˜ ì§§ì€ ë³€ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ì•™ ì •ì‚¬ê°í˜• í¬ê¸° ì„¤ì •

  const tempCanvas = document.createElement("canvas");
  tempCanvas.width = size;
  tempCanvas.height = size;
  const tempCtx = tempCanvas.getContext("2d");

  const sx = (video.videoWidth - cropSize) / 2;
  const sy = (video.videoHeight - cropSize) / 2;

  // ì¤‘ì•™ ì˜ì—­ ìë¥´ê¸° ë° ë¦¬ì‚¬ì´ì¦ˆ
  tempCtx.drawImage(video, sx, sy, cropSize, cropSize, 0, 0, size, size);

  const imgData = tempCtx.getImageData(0, 0, size, size).data;
  const float32 = new Float32Array(1 * 3 * size * size);
  for (let i = 0; i < size * size; i++) {
    float32[i] = (imgData[i * 4] / 255) * 2 - 1; // R
    float32[i + size * size] = (imgData[i * 4 + 1] / 255) * 2 - 1; // G
    float32[i + 2 * size * size] = (imgData[i * 4 + 2] / 255) * 2 - 1; // B
  }

  return new ort.Tensor("float32", float32, [1, 3, size, size]);
}


function handleDetections(detections) {
  const banner = document.getElementById("banner");
  if (!banner) return;

  banner.style.display = detections.length > 0 ? "block" : "none";
}

function drawMask() {
  const w = canvas.width;
  const h = canvas.height;

  const boxSize = Math.min(w, h) * 0.6;
  const verticalOffset = 0;
  const left = (w - boxSize) / 2;
  const top = (h - boxSize) / 2 + verticalOffset;

  // âœ… 1. ë§¤ í”„ë ˆì„ clear
  ctx.clearRect(0, 0, w, h);

  // âœ… 2. ì „ì²´ ë°˜íˆ¬ëª… ê²€ì€ìƒ‰ ë®ê¸°
  ctx.save();
  ctx.fillStyle = "rgba(0, 0, 0, 0.75)";
  ctx.fillRect(0, 0, w, h);

  // âœ… 3. ì¤‘ì•™ ì‚¬ê°í˜• ì˜ì—­ì„ íˆ¬ëª…í•˜ê²Œ ëš«ìŒ
  ctx.globalCompositeOperation = "destination-out";
  ctx.beginPath();
  ctx.rect(left, top, boxSize, boxSize);
  ctx.fill();

  // âœ… 4. ë‹¤ì‹œ ì„ ì„ ê·¸ë¦´ ìˆ˜ ìˆë„ë¡ ë³µêµ¬
  ctx.globalCompositeOperation = "source-over";
  ctx.strokeStyle = "white";
  ctx.lineWidth = 2;
  ctx.strokeRect(left, top, boxSize, boxSize);

  ctx.restore();
}



function postprocess(outputTensor) {
  const logit = outputTensor.data[0];
  const score = 1 / (1 + Math.exp(-logit)); // Apply sigmoid to get probability
  const threshold = 0.5; // Adjust as needed

  const isEcoqcodeDetected = score > threshold;
  console.log("ECOQCODE Detection Score:", score, "Detected:", isEcoqcodeDetected);
  return isEcoqcodeDetected;
}


async function detectLoop() {
  if (!session) return;

  const tensor = preprocessFrame();
  const outputMap = await session.run({ input: tensor });

  const outputName = session.outputNames[0];
  const outputTensor = outputMap[outputName];

  const isEcoqcodeDetected = postprocess(outputTensor);
  drawMask();
  handleDetections(isEcoqcodeDetected);

  requestAnimationFrame(detectLoop);
}

async function main() {
  await initCamera();
  await loadModel();

  // ì˜ìƒ ì •ë³´ í™•ë³´ë¥¼ ìœ„í•œ ëŒ€ê¸° (1í”„ë ˆì„ ë’¤ì—ì•¼ width/height ì ‘ê·¼ ê°€ëŠ¥)
  await new Promise(resolve => setTimeout(resolve, 500));

  // ğŸ“Œ ìº”ë²„ìŠ¤ í¬ê¸° ì„¤ì •
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;

  detectLoop();
}


main();
