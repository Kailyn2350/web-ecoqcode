{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECOQCODE OCR CNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation (Synthetic Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "bg_dir = './backgrounds'\n",
    "out_dir = './eco_dataset/images'\n",
    "label_file = './eco_dataset/labels.txt'\n",
    "font_path = 'arial.ttf'  # Change as per your system (e.g., 'NotoSansCJK-Regular.ttc')\n",
    "text = 'ECOQCODE'\n",
    "num_samples = 1000  # Number of images to generate\n",
    "image_size = (224, 224)  # Output image size\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# List of background image files\n",
    "bg_files = [str(p) for p in Path(bg_dir).glob('*') if p.suffix.lower() in ['.jpg', '.png', '.jpeg']]\n",
    "\n",
    "# Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(font_path, size=24)\n",
    "except:\n",
    "    raise RuntimeError(f\"Font file not found: {font_path}\")\n",
    "\n",
    "# Save labels\n",
    "with open(label_file, 'w', encoding='utf-8') as lf:\n",
    "    for i in range(num_samples):\n",
    "        bg_path = random.choice(bg_files)\n",
    "        img = Image.open(bg_path).convert(\"RGB\").resize(image_size)\n",
    "\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Set label for binary classification\n",
    "        has_text = random.random() < 0.5  # 50% chance to insert ECOQCODE\n",
    "        label = 1 if has_text else 0\n",
    "\n",
    "        if has_text:\n",
    "            # Determine text size\n",
    "            for _ in range(10):  # Retry 10 times if text is larger than image\n",
    "                text_size = random.randint(6, 12)\n",
    "                font = ImageFont.truetype(font_path, text_size)\n",
    "                text_width = int(text_size * 0.6 * len(text))  # Approximate width calculation\n",
    "                text_height = text_size\n",
    "\n",
    "                if text_width < image_size[0] and text_height < image_size[1]:\n",
    "                    break\n",
    "            else:\n",
    "                # Skip if text cannot be inserted\n",
    "                continue\n",
    "\n",
    "            x = random.randint(0, image_size[0] - text_width)\n",
    "            y = random.randint(0, image_size[1] - text_height)\n",
    "\n",
    "            draw.text((x, y), text, font=font, fill=(0, 0, 0))\n",
    "\n",
    "        # Save file\n",
    "        filename = f\"img_{i:05d}.jpg\"\n",
    "        save_path = os.path.join(out_dir, filename)\n",
    "        img.save(save_path)\n",
    "\n",
    "        # Record label\n",
    "        lf.write(f\"{save_path}\\t{label}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79ff46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class ECOQDataset(Dataset):\n",
    "    def __init__(self, label_path, transform=None):\n",
    "        self.samples = []\n",
    "        with open(label_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                path, label = line.strip().split(\"\\t\")\n",
    "                self.samples.append((path, int(label)))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15126ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)  # RGB Normalization\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture (CNN Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7489e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ECOQClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze(1)  # BCEWithLogitsLoss expects shape (N,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting (Train/Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0978e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_file = \"./eco_dataset/labels.txt\" \n",
    "\n",
    "# Load label list and split 8:2\n",
    "with open(label_file, encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "train_lines, val_lines = train_test_split(lines, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Split files\n",
    "with open(\"eco_dataset/train_labels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_lines)\n",
    "\n",
    "with open(\"eco_dataset/val_labels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(val_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset and DataLoader Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def18c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ECOQDataset(Dataset):\n",
    "    def __init__(self, label_file, transform=None):\n",
    "        with open(label_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        self.samples = [line.strip().split('\\t') for line in lines]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor([int(label)], dtype=torch.float32)\n",
    "        return image, label\n",
    "\n",
    "# Set data paths\n",
    "train_label_path = './eco_dataset/train_labels.txt'\n",
    "val_label_path = './eco_dataset/val_labels.txt'\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = ECOQDataset(train_label_path, transform=transform)\n",
    "val_dataset = ECOQDataset(val_label_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb1440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 25/25 [00:01<00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.7132\n",
      "Validation Accuracy: 50.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 25/25 [00:01<00:00, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.6917\n",
      "Validation Accuracy: 61.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 25/25 [00:01<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.6475\n",
      "Validation Accuracy: 75.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 25/25 [00:01<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.5168\n",
      "Validation Accuracy: 79.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 25/25 [00:01<00:00, 22.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.3858\n",
      "Validation Accuracy: 84.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 25/25 [00:01<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.2668\n",
      "Validation Accuracy: 88.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 25/25 [00:01<00:00, 21.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.1962\n",
      "Validation Accuracy: 88.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 25/25 [00:01<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.1727\n",
      "Validation Accuracy: 88.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 25/25 [00:01<00:00, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.1708\n",
      "Validation Accuracy: 88.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 25/25 [00:01<00:00, 21.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.1325\n",
      "Validation Accuracy: 91.50%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ECOQClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        imgs, labels = imgs.to(device), labels.float().to(device)\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, labels.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device).float().squeeze()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            preds = (torch.sigmoid(outputs).squeeze() > 0.5).float()\n",
    "\n",
    "            # Adjust shape if necessary\n",
    "            if preds.dim() != labels.dim():\n",
    "                labels = labels.squeeze()\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Validation Accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation and ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d88ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8534    1.0000    0.9209        99\n",
      "         1.0     1.0000    0.8317    0.9081       101\n",
      "\n",
      "    accuracy                         0.9150       200\n",
      "   macro avg     0.9267    0.9158    0.9145       200\n",
      "weighted avg     0.9275    0.9150    0.9145       200\n",
      "\n",
      "Model successfully exported to ecoq_classifier.onnx\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device).float().squeeze()\n",
    "        outputs = model(imgs)\n",
    "        preds = (torch.sigmoid(outputs).squeeze() > 0.5).float()\n",
    "\n",
    "        if preds.dim() != labels.dim():\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# Export the trained model to ONNX format\n",
    "output_onnx_path = \"ecoq_classifier.onnx\"\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device) # Example input: batch_size=1, channels=3, height=224, width=224\n",
    "\n",
    "torch.onnx.export(model,                   # trained model\n",
    "                   dummy_input,             # example input for tracing\n",
    "                   output_onnx_path,        # where to save the ONNX model\n",
    "                   export_params=True,      # store the trained parameter weights inside the model file\n",
    "                   opset_version=11,        # the ONNX version to export the model to\n",
    "                   do_constant_folding=True, # whether to execute constant folding for optimization\n",
    "                   input_names = ['input'],   # the model's input names\n",
    "                   output_names = ['output'], # the model's output names\n",
    "                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                 'output' : {0 : 'batch_size'}})\n",
    "\n",
    "print(f\"Model successfully exported to {output_onnx_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
